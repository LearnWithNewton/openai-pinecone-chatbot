Chapter: Introduction
Topic: Building context-aware AI tools with custom knowledge bases
Transcript:  Ever wonder how AI tools respond with such relevance or how platforms like Spotify, Netflix, and YouTube provide personalized experiences by predicting our preferences with uncanny accuracy? Now, imagine building a chatbot that not only answers questions but gets what you're really asking about. This course is your guide to developing AI conversations that feel more human by building chatbots that can retain context and pull information from a custom knowledge base. I'm Guil Hernandez, a software development instructor and developer for over 15 years, and I'm excited to start this learning journey with you. So if you're ready to blend the capabilities of open AI models with the speed and efficiency of a vector database to create chat applications that truly resonate with users, I invite you to take this course.

Chapter: Introduction
Topic: What you should know about AI and JavaScript
Transcript: - Before we get started, I want to make sure you're set up to hit the ground running and get the most out of this course. This course is designed for those who already have a solid foundation in JavaScript and are familiar with working with APIs. Specifically, you'll build what's called a RAG, or retrieval augmented generation chatbot, using the OpenAI API and the Pine Cone Vector database. You'll also run your project locally with Vite, a modern and fast frontend tool that simplifies project setup and development. To fully benefit from this course, you should also be comfortable with asynchronous programming in JavaScript and experience with the OpenAI APIs Chat completions endpoint is also a plus, as it will help you grasp and manage the chatbot messages and responses more quickly. If you're new to APIs or need a refresher on JavaScript, I recommend brushing up on those skills first. This will ensure you can follow along smoothly. If you're ready to go, let's keep going.

Chapter: Introduction
Topic: Set up the project
Transcript: - In this course, you'll gain a deeper understanding of generative AI by building a context-aware chat tool that answers questions for users interested in attending a fictional conference named Red 30 Tech Conf. You can access the project files for this course by following the link to the GitHub repository posted with this video. The files are organized by chapter and lesson for ease of use. Each branch of the repository contains the beginning or end state of a lesson, allowing you to use them as references and follow along with the course. To get started, download the project files from the main branch or the files on the 00_3b branch. I organize the files by chapter and video. The files for each video contain a beginning and end state you can use as a reference and follow along with me. Make sure that you have node installed on your machine to install all the project dependencies and run the chat bot app, which I've set up using the Vite build tool and dev environment. Once you open the project files in a text editor like VS Code, open your terminal and run npm install to install the project dependencies. Then run npm run dev to run the app and launch it in your browser. And this is what the tool looks like to start. It doesn't do much yet, but that's where you come in to bring it to life. Before getting started, I want to review a few details related to the project files. Throughout the course, you'll work with various OpenAI models using the OpenAI API, which you'll conveniently access via their node API library. In the file config.js, I'm importing the OpenAI node API library or SDK, which I've installed as a dependency. I've also kicked off the configuration for making requests to the OpenAI API. To follow along with the course and complete the exercises, you'll need an OpenAI API key, which you'll provide here where I'm initializing a new instance of OpenAI. And since you'll build this app using plain JavaScript and no server side code or frameworks, I've added this dangerouslyAllowBrowser flag and set it to true to indicate that the node library should be allowed to run in a browser environment. But do keep in mind that I'm only using this for local development and learning purposes. It's important that you keep your API keys secure. Do not expose them in client side code or public repositories. And you'll be writing most of your code here in the file main.js, which is importing the OpenAI configuration from config.js, and the file style.css contains all of the CSS used to style the project, but you won't need to update anything in this file unless you'd like to customize the design. Okay, that's all for the project setup. You are now ready to go. So join me in the next video to start learning about a core concept in generative AI models and applications called embeddings.

Chapter: Text Embeddings
Topic: The importance of embeddings in generative AI
Transcript: Generative AI and AI powered tools are transforming the world. But what powers this innovation? And how do platforms like Spotify, Netflix, and YouTube provide personalized experiences to users? Part of it is something called embeddings, and embeddings are at the core of many AI tools, enabling them to understand and translate data into meaningful experiences. Grasping this concept is essential for anyone learning to build and work with AI. So first, let's get into what embeddings are, specifically text embeddings, which is the focus of this course. An embedding is a vector representation of text, be it a word, sentence, or document that allows AI models to process language in a way that captures the semantic meaning of words and phrases, and the relationships between other words and phrases. AI is really good at understanding numbers. This numerical form is what allows models to understand text in a human-like way, but through mathematical relationships versus intuitive understanding. These numbers are not random. They're learned from data. AI models are trained using massive amounts of text. They can figure out patterns and relationships between words, and they encode this information into the embeddings. Imagine vector embeddings as points on a graph. The vectors are carefully calculated so that vectors representing words with similar meanings get plotted closer together, while unrelated words are farther apart. Behind the scenes, special algorithms are at work measuring the distance between vectors to assess the closeness or similarity between these points. This measurement helps determine how closely related to words or concepts are. That's how AIs are able to understand language and how words relate to each other. For example, a support ChatBot designed for a tech conference might get questions like, what time is the keynote? Or who's presenting in the AI panel? Through embeddings, the ChatBot can understand that these inquiries are about session schedules or speaker details. Most importantly, embeddings help AI figure out the context, like whether the word bark refers to a tree or a dog based on the conversation. This makes AI seem more human, able to chat with us, suggest songs or recommend movies and shows that we'll enjoy. As you've learned, embeddings convert text into numerical vectors that AI models can easily work with helping interactions with them feel more human. With this knowledge, you're ready to learn more about the technical details of embeddings, and how they are created and used in AI powered apps.

Chapter: Text Embeddings
Topic: Create embeddings with OpenAI
Transcript: Embeddings are numerical vectors that help AI understand what words mean and how they relate to each other. They're a big part of what makes AI-powered tools smart, performing common tasks like searching, comparing, recommending, generating text and more. So how are embeddings created and how do generative AI models use them? Well, I'll start by teaching you how to create embeddings using the OpenAI API. OpenAI provides powerful and highly trained models to generate embeddings. The process involves feeding a word, sentence, or entire document into the model which then returns a vector that captures the text's essence, meaning, and context. That's exactly what you're going to start doing now. So let's get hands on and see how this works in practice. First, make sure that you have your API key from OpenAI to start. Once you have that, you can use OpenAI's embedding model. All right, so to generate embeddings with the OpenAI API, you send text to the embeddings API endpoint specifying which embedding model to use. Currently OpenAI offers three embedding models as you can view here in the docs with text-embedding-3-large and text-embedding-3-small being their latest third generation models. For this course, I'm going to use the model text-embedding-3-small which has an output dimension of 1,536. More on that part in just a bit. You'll notice how the API docs provide examples and code snippets for creating and embedding what's JavaScript using its node API library or SDK which I've already installed as a dependency in our project. If you have a look at the project files package.json file, you can see that OpenAI has been installed as a dependency. Okay, so this snippet here will get me started with generating embedding. So what I'll do is copy this main function right from the docs and paste it in my text editor, here in the file main.js. Now currently this snippet is using the embedding model text-embedding-ada-002. So I'll update it to use the newer model which is text-embedding-3-small just like that. The input text is currently a string which I'll update to say, "Hi, I'm your teacher, Guil!" And you can optionally set the format to return the embeddings using this encoding format parameter. By default, it's set to float, but you can also set it to something like base64, but I'll keep it as float for this course. And what this means is that it's going to return the embeddings as floating point numbers as you'll see soon. And this main function logs the embedding response to the console, so I'll run this code and open up the developer tools, and notice how in the console we get back a response object containing the data with the embedding. Now this response holds a lot of useful information, as you'll see, like the token usage and model, but what I'm looking for is this part here, the embedding vector. As you can see, it's a regular JavaScript array holding a massive list of floating point numbers. So just so we can have a better look at it, I'll access and log only the embedding portion of the response with embedding.data. I want to access the object at index zero and it's embedding property. And there it is, the actual embedding for the text, "Hi, I'm your teacher, Guil!" Now something that's really important to know about embeddings is the concept of dimensions or how many numbers are inside the embeddings array. As I mentioned, the text-embedding-3-small model I'm using supports a dimension of 1,536. So yes, this means that there are that many floating point numbers inside this array which you can confirm here in the array length. And actual text input size doesn't matter at all. So for example, if I changed this input to just, "Hi," you'll notice how the embedding array's length or dimension is still 1,536. Now these numbers may not mean much to us, but for AI, this embedding array holds important semantic details about the input text. In this case, all of the different semantic or contextual aspects of the words hi. And if I use the larger model, text-embedding-3-large, for example, it would mean double that, a dimension size of 3,072 as you can see here in the docs. Now OpenAI's embedding models are not just for single sentences. You can also pass it an array of strings and it will generate an embedding for each string. So I'll go ahead and try this with an array of conference session descriptions. So now I'll need to pass this session's array as the input like so and then I'll log the value of the data array to the console just like that. So what the function is going to do is send this array of strings to the embedding model and then return a set of high dimensional vectors for each string. Let's have a look. Okay, so now in the console, I see a list of five embedding objects each containing an embedding array, nice. So with OpenAI's embedding model, a word, sentence, or even entire documents can be reduced to a vector which can be used to compare text, search through large volumes of text, or even feed it into another AI model to generate responses or summaries. Now it is important to be mindful of token usage when using OpenAI's embedding models. Each piece of text you input into the model consumes tokens. So do check out their pricing page to learn more about the latest information on embeddings pricing. All right, so well done. You've just unlocked a fundamental skill in generative AI, creating embeddings with the OpenAI API.

Chapter: Text Embeddings
Topic: Pair embeddings with related text
Transcript: You've learned some of the basics of embeddings and even created your own using an OpenAI embedding model. Currently you're left only with the embedding data for each string in the sessions array. So how do you actually use this data or feed it to an AI? Well, first pairing each embedding with its corresponding text chunk would be useful. This will also help when you begin storing these vectors in a database which you'll learn more about later in this course. So this will involve iterating over the data in the sessions array and placing each string in an object with its corresponding embedding, and this time, I've added more session titles to the array. Notice how there are 10 strings consisting of session titles. Now there are various ways you might do this. I'll cover one approach using the JavaScript for-of loop. First, inside this main function, I'll initialize a data variable to an empty array. Then right below, set up my for-of loop with for const, let's say textChunk of text, and this main function will accept a parameter named text. Then I'll place the call to the OpenAI embeddings endpoint within my for-of loop just like that. Once again, I'll use the text-embedding-3-small model. Then set the value of input to the current value of textChunk in the for-of iteration and keep the encoding format set to float. Now through each iteration of the for loop, we're going to push a object into our data array. So right below we'll say data.push and pass it an object, and this object is going to have two properties. One will be named content and this will be set to the current textChunk value, and then we'll have a values property which will be the embedding. And the way we can access the embedding is again, with embeddingResponse.data, and I'll access the object at index zero, and it's embedding property just like that. All right, right below, I'll console.log the value of data and then just to make sure that everything went okay, I'll log a message to the console that says Embeddings complete. All right, so this approach processes each input textChunk sequentially within the loop, ensuring that only one API request gets handled at a time which can be useful for small data sets and managing API rate limits, for example. So finally, I'll call my main function and pass it my sessions array as the input, run the code, and let's have a look. Good, I get the the Embeddings complete message. I'll open up the array and the console and there you have it, each embedding alongside its respective textChunk. So in the first object, we have the values properties set to the embedding, for the session title, Welcome Ceremony and Keynote. Then in the second object, we have the embedding for AI and Education, Developing a Data Strategy, and so on. As I mentioned earlier, this pairing brings structure to the data, preparing it for other uses like storing and managing these pairs in a database.

Chapter: Text Embeddings
Topic: Text chunking for embeddings
Transcript: So far, you've created embeddings from short sentences or session titles. Well, working with a large amount of text or even entire documents poses a challenge, especially when your goal is to capture as much context as possible in a piece of text. For instance, in the latest project files, I've included a text file in this documents folder named sessions.txt, and it contains more detailed information about the Red30 Tech conference, such as the session dates, prices, lodging information, and all the sessions and speakers. Now, generating a single embedding for an entire document like this can result in the loss of critical nuanced details within the context or details that are crucial for the AI's understanding and interpretation. Also, most text embedding models like those provided by OpenAI have a token limit. OpenAI's limit, for instance, is 8,191 tokens, which is equivalent to about 5500 words. So it's best to break up large amounts of text into smaller individual chunks so that you don't exceed these limits. And there are many tools out there that can chunk documents for you. I'll teach you how to split text using LangChain, which is a popular framework in AI development, and you can learn a whole lot more about it at langchain.com. Now, before chunking, it is important to pre-process your data. So this means cleaning up your text to remove any irrelevant information like HTML tags or miscellaneous symbols, if you're copying your content from a website, for instance, also correcting typos and standardizing the formatting. Now, LangChain offers several tools for splitting your text, and one useful splitter is the CharacterTextSplitter, which lets you split text based on specific characters called a separator, and then it counts chunk length by the number of characters you set. And overall, it provides a really simple way to create manageable chunks for the embedding model. For this course, I'll use LangChain's RecursiveCharacterTextSplitter, which is much better suited for more complex documents. What it does is it iteratively splits your text into optimal sizes while trying to keep semantically related sentences and paragraphs together as long as possible. To use this text splitter, I'll need to install LangChain in my project. So in the terminal, I'll go ahead and stop running the project, then run npm install langchain. All right, I'll run my project again with npm run dev. There we go. And then I'll import the RecursiveCharacterTextSplitter from LangChain here in my main digest file. In fact, you can refer to the LangChain documentation for that code, which is this import statement here. So I'll copy that and paste it into Main Digest. And for this, I'll go ahead and remove the sessions array from my file, because now we are going to be bringing in the text from the sessions.txt file here in the documents folder. All right, so this code snippet creates a new instance of RecursiveCharacterTextSplitter and then splits the text you provide. To start, I've defined a function in the latest project files named split Text. So what I'll do is copy this entire splitter snippet into my function. And the text splitter configuration includes the chunk size and chunk overlap parameters. The chunk size influences how much text each chunk contains, and the chunk overlap ensures continuity and context between chunks with a bit of text from the end of one chunk repeated at the beginning of the next, as you'll see soon. So what I need to do first is access the content from the document I want to split, which is the sessions.txt file. Now for this front end project, I'll use the fetch method with await fetch to make a request to get the file sessions.txt. So I'll pass fetch a string containing the path to the sessions.txt file, and I'll save the fetch response in a const named response. All right, next, I'll convert the response object into plain text with await response text, and I'll store the resulting text from sessions.txt and a constant named text. Okay, now let's move on to the splitter configuration. What I'll do for this project is set the chunk size to a small value like 125, and the chunk overlap to 15. So this means that it's going to take the last 15 characters of one chunk, and it's going to repeat those characters at the beginning of the next chunk. And it does that to help preserve some context and continuity between chunks. The output variable is going to hold the split text, so to test this, I'll log the value of output to the console. Finally, I'll call my split text function, then run my code and head over to the browser and check out the console. All right, so I'll view the app in the browser, open up the console, and good. Notice how it logs an array with 55 document objects. Each has a page content property containing the text chunk, and also a metadata property, which lets you know the location of the text chunk in the original document, which could be useful in some cases. And you'll notice how each chunk is roughly 155 characters, including the character overlaps or the text shared between adjacent chunks. Now with the RecursiveCharacterTextSplitter, your chunks won't all exactly be the chunk size and overlap you've set, but the splitter does try to make them consistent in size. Selecting the best chunk size and overlap depends on several factors, like the embedding model limitations, the expected user queries, and how you plan to use the text chunks. So I always recommend experimenting to find the right settings for your project. Aim to produce the smallest chunk size possible while preserving as much context or information. So for instance, if you can look at a text chunk and make sense of it without surrounding context, there's a good chance that a language model can understand it too. Text chunking is important, especially when dealing with volumes of text. As you'll learn by breaking down documents into smaller chunks, AI models can better understand and capture the context and nuances of the text.

Chapter: Vector Databases
Topic: The power of vector databases
Transcript: You've taken your first steps in transforming sentences and even entire documents into text embeddings using tools like LangChain and powerful embedding models like those offered by OpenAI. So now you're dealing with a massive number of complex embeddings, remember, each with a dimensionality of 1536, and you need a place to store these. Normally, when developing data-powered apps, you'll store data in a MySQL or Postgres database. But traditional databases like these aren't necessarily equipped to handle such complex, high-dimensional data efficiently. The solution? Vector databases. Unlike traditional databases that rely on exact keyword matching to retrieve information, vector databases use advanced similarity metrics to quickly find vectors in the database that are closest to a given vector. Vector databases use all the information encoded in embeddings to determine how closely related two vectors are based on their dimensions and values. This allows AI systems to find the most relevant vectors to a user query with extraordinary accuracy, matching content using all of the information encoded in embeddings. So what is the role and significance of embeddings and vector databases in generative AI tools and applications? Well, even the most advanced AI models, especially those based on large language models like GPT, have a limitation. They're only as good as the data they were last trained on. This means there could be gaps in their knowledge, which can lead to outdated or imprecise responses, not to mention the occasional tendency to hallucinate or generate incorrect or nonsensical information under certain conditions. So vector databases address this by letting you store custom, up-to-date information as embeddings. That way your AI applications can provide responses tailored to the latest and most relevant information, further augmenting the capabilities of pre-trained or foundational models. So now imagine the possibilities of building AI-powered tools that are well-informed about a specific domain or custom dataset, like a contract, legal document, your company product details, and more. Vector databases are a big deal in the world of generative AI. They do all the heavy lifting and math calculations to help developers make the most of embeddings, searching and finding data based on its context and meaning, not just exact matches.

Chapter: Vector Databases
Topic: Set up a vector database with Pinecone
Transcript: You learn that vector databases store and query embeddings quickly end at scale. In this course I'll use Pinecone, A vector database designed to handle high dimensional vectors like the ones you've created. Pinecone is ideal for building AI-powered apps, requiring fast and accurate searches, like recommendation engines, chatbots, user support tools, and more. To begin, head over to pinecone.io and sign up for an account. Pinecone offers various plans to suit different needs, including a free tier that's perfect for getting started and experimenting with vector databases. I've signed up for the free plan and recommend you do as well for now. Once you've signed up, you'll gain access to the Pinecone Console where you can create and manage vector databases. Pinecone indexes store records with vector data. To create your first index directly in the Pinecone console, click Create Index. Start by giving the index a name like red30-tech-conf notice how the name can only contain lowercase letters, numbers, and hyphens. When configuring your index, the dimensions must be consistent with the embeddings you store. For instance, clicking setup by model shows that open AI's text embedding model has a dimensionality of 1,536, so enter 1,536 for dimensions. Pinecone uses a distance metric to calculate the similarity between vectors. I see that open AIs model requires a co-sign similarity metric. So select co-sign for the metric value. You'll learn more about this metric shortly. Under capacity mode, you'll have a serverless index by default, if you are on the free tier. Serverless indexes scale automatically based on usage, and you pay only for the amount of data stored and operations performed. Now, currently with the free tier, you get up to two gigabytes of storage, which is more than enough to get you started without paying. And by default, the cloud provider is AWS, and it's going to choose a region closest to you when on the free starter plan. Finally, click Create Index. Then your first index should be up and running shortly. There we go, it's all set up. Okay, next, you'll need to connect your application to your index, so you can store and query vectors from the app. You'll need an API key for that. So click API keys and the sidebar nav, and you'll see that Pinecone has already set a default key to get started. Be sure to keep this API key handy because soon you'll use it along with these code snippets shown here to install Pinecones node client in your project, initialize it, and then write vectors into your index.

Chapter: Vector Databases
Topic: Store embeddings in Pinecone
Transcript: You've set up a Pinecone account and index, the next step is to store embeddings in Pinecone in preparation for something called similarity searches. Pinecone offers a REST API for storing and querying your vector data. But to streamline your development process, I recommend using Pinecone's node client. It provides the easiest way to create and interact with indexes. First, be sure to download the very latest project files with this video, where I've already installed and set up the Pinecone library and the project using the install and initialize snippets provided by Pinecone. So with your API key in hand, you'll need to initialize your Pinecone client here. And I'm also targeting my red30-tech-conf index here and importing Pinecone in my main.js file as well as the Pinecone index for red30-tech-conf. Okay, so you've already created an index through the Pinecone dashboard earlier. Now, you can test your connection to Pinecone using the listIndexes method, which returns information about all indexes in your project. So I'll just test it here with console.log, then call it with await pinecone.listIndexes. And in the console I get back information about my only index, red30-tech-conf. I see the index name, dimension, metric, and most importantly, that it's ready to go. Now remember, with the free starter plan, you get one project and up to five indexes for that project. And the starter plan also does not support all Pinecone features, which I'll talk more about soon. All right, now I'll generate new embeddings from text chunks, again, using the text in the sessions.txt file and then store them in Pinecone. This splitText function should look familiar. It's going to split the text in the provided text file or document using LangChain's RecursiveCharacterTextSplitter. This time I'm using a chunk size of 200 characters and a chunk overlap of 20 characters. Next, the generateEmbeddings function will loop over the array of text chunks, passing each to OpenAI's embedding endpoint to convert each chunk into an embedding. And again, as you've seen in previous videos, I'm pairing each text chunk with its corresponding embedding. Now, when storing vectors in Pinecone, you need to structure each vector set like so, a unique ID that's a string, and a values property, holding the vectors. Pinecone also lets you attach metadata key value pairs to vectors. So I'll add a property named metadata and set it to an object holding the content property containing the current text chunk. And there are various ways you might generate a unique ID. What I did for this lesson here in the utils folder is create a simple function that increments by one each time it's run and then returns the number as a string. I'm already importing the function here in my main.js file, so I can call it as the value of the ID property, just like that. And this generateEmbeddings function is going to return the final data array. And now I can begin storing vectors in my index using Pinecone's upsert method. Upsert is the process of inserting records if they don't exist or updating them if they do. So here's how you can upsert a batch of vectors into your index. I'll start by defining an async function named upsertRecords. Inside the function is where I'll call my generateEmbeddings function with await generateEmbeddings, and I'll pass it the path to the file I want to create embeddings from. So that is this file here. Remember it's /documents/sessions.txt, which I can reference with my txtFile variable. And I'll store the embeddings and a const named embeddingsData. Next, I'll upsert the records into my Pinecone index with await, then call index.upsert and then I'll pass it my embeddingsData, just like that. And finally, just to make sure that the upserting was successful, I'll log a message to the console that says, Upsert Successful. All right, now I'll call my upsertRecords function to upsert all of the records into my index. I'll run my code and let's have a look at the console. Okay, we get the message Embeddings complete! So that part went well and good. I see the console message, Upsert Successful! After adding or updating records, I recommend checking that your vectors are correctly stored and available for querying. Now, there might be a brief delay before you can access all of the latest vector data, but you can verify the current state using Pinecone's describeIndexStats method. So here I'll quickly console.log that with await, then say, index.describeIndexStats. And that's going to return stats about everything stored in your index. You can also quickly verify that your latest upsert was successful by checking the Pinecone console. And it was, good! Here I see that the vector count is 39 and I can view all of the stored records below. Perfect. Now, when dealing with multiple records that represent chunks of a single parent, like in my project, it helps to prefix each records ID with a reference to that parent doc. For example, name your IDs like so, red30TechConf#1, red30TechConf#2, #3, #4, and so on. And keep in mind that when inserting or updating larger amounts of data, Pinecone recommends inserting records in batches of 100 or fewer over multiple requests. Now, in my case, I inserted less than 100 records in this case, 39, so I'm okay. And earlier I mentioned that Pinecone's free starter plan doesn't support all Pinecone features. If you're curious, some of the features include configuring your index to scale horizontally and vertically, and choosing different regions for your environments. Also, Pinecone indexes all metadata by default, meaning it organizes and prepares the metadata to make it quickly searchable and accessible using metadata filtering. And when metadata contains many unique values, like your text chunks, they'll likely consume more memory. So another feature that is not available in the starter plan is keeping memory utilization low with a feature called selective metadata indexing. Please review the Pinecone's documentation to learn a whole lot more about this. For now, keep your chunk sizes small. It's not so much of an issue with the document and index size you're using now while learning these concepts. And you can always upgrade your Pinecone plan when you're ready. Okay, so with your data indexed, you're now ready to query vectors and perform similarity searches.

Chapter: Vector Databases
Topic: The power of vector databases
Transcript:

Chapter: Vector Databases
Topic: The power of vector databases
Transcript:

Chapter: Vector Databases
Topic: The power of vector databases
Transcript:

