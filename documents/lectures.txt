Chapter: 0. Introduction
Topic: Building context-aware AI tools with custom knowledge bases
Transcript:  Ever wonder how AI tools respond with such relevance or how platforms like Spotify, Netflix, and YouTube provide personalized experiences by predicting our preferences with uncanny accuracy? Now, imagine building a chatbot that not only answers questions but gets what you're really asking about. This course is your guide to developing AI conversations that feel more human by building chatbots that can retain context and pull information from a custom knowledge base. I'm Guil Hernandez, a software development instructor and developer for over 15 years, and I'm excited to start this learning journey with you. So if you're ready to blend the capabilities of open AI models with the speed and efficiency of a vector database to create chat applications that truly resonate with users, I invite you to take this course.

Chapter: 0. Introduction
Topic: What you should know about AI and JavaScript
Transcript: - Before we get started, I want to make sure you're set up to hit the ground running and get the most out of this course. This course is designed for those who already have a solid foundation in JavaScript and are familiar with working with APIs. Specifically, you'll build what's called a RAG, or retrieval augmented generation chatbot, using the OpenAI API and the Pine Cone Vector database. You'll also run your project locally with Vite, a modern and fast frontend tool that simplifies project setup and development. To fully benefit from this course, you should also be comfortable with asynchronous programming in JavaScript and experience with the OpenAI APIs Chat completions endpoint is also a plus, as it will help you grasp and manage the chatbot messages and responses more quickly. If you're new to APIs or need a refresher on JavaScript, I recommend brushing up on those skills first. This will ensure you can follow along smoothly. If you're ready to go, let's keep going.

Chapter: 0. Introduction
Topic: Set up the project
Transcript: - In this course, you'll gain a deeper understanding of generative AI by building a context-aware chat tool that answers questions for users interested in attending a fictional conference named Red 30 Tech Conf. You can access the project files for this course by following the link to the GitHub repository posted with this video. The files are organized by chapter and lesson for ease of use. Each branch of the repository contains the beginning or end state of a lesson, allowing you to use them as references and follow along with the course. To get started, download the project files from the main branch or the files on the 00_3b branch. I organize the files by chapter and video. The files for each video contain a beginning and end state you can use as a reference and follow along with me. Make sure that you have node installed on your machine to install all the project dependencies and run the chat bot app, which I've set up using the Vite build tool and dev environment. Once you open the project files in a text editor like VS Code, open your terminal and run npm install to install the project dependencies. Then run npm run dev to run the app and launch it in your browser. And this is what the tool looks like to start. It doesn't do much yet, but that's where you come in to bring it to life. Before getting started, I want to review a few details related to the project files. Throughout the course, you'll work with various OpenAI models using the OpenAI API, which you'll conveniently access via their node API library. In the file config.js, I'm importing the OpenAI node API library or SDK, which I've installed as a dependency. I've also kicked off the configuration for making requests to the OpenAI API. To follow along with the course and complete the exercises, you'll need an OpenAI API key, which you'll provide here where I'm initializing a new instance of OpenAI. And since you'll build this app using plain JavaScript and no server side code or frameworks, I've added this dangerouslyAllowBrowser flag and set it to true to indicate that the node library should be allowed to run in a browser environment. But do keep in mind that I'm only using this for local development and learning purposes. It's important that you keep your API keys secure. Do not expose them in client side code or public repositories. And you'll be writing most of your code here in the file main.js, which is importing the OpenAI configuration from config.js, and the file style.css contains all of the CSS used to style the project, but you won't need to update anything in this file unless you'd like to customize the design. Okay, that's all for the project setup. You are now ready to go. So join me in the next video to start learning about a core concept in generative AI models and applications called embeddings.

Chapter: 1. Text Embeddings
Topic: The importance of embeddings in generative AI
Transcript: Generative AI and AI powered tools are transforming the world. But what powers this innovation? And how do platforms like Spotify, Netflix, and YouTube provide personalized experiences to users? Part of it is something called embeddings, and embeddings are at the core of many AI tools, enabling them to understand and translate data into meaningful experiences. Grasping this concept is essential for anyone learning to build and work with AI. So first, let's get into what embeddings are, specifically text embeddings, which is the focus of this course. An embedding is a vector representation of text, be it a word, sentence, or document that allows AI models to process language in a way that captures the semantic meaning of words and phrases, and the relationships between other words and phrases. AI is really good at understanding numbers. This numerical form is what allows models to understand text in a human-like way, but through mathematical relationships versus intuitive understanding. These numbers are not random. They're learned from data. AI models are trained using massive amounts of text. They can figure out patterns and relationships between words, and they encode this information into the embeddings. Imagine vector embeddings as points on a graph. The vectors are carefully calculated so that vectors representing words with similar meanings get plotted closer together, while unrelated words are farther apart. Behind the scenes, special algorithms are at work measuring the distance between vectors to assess the closeness or similarity between these points. This measurement helps determine how closely related to words or concepts are. That's how AIs are able to understand language and how words relate to each other. For example, a support ChatBot designed for a tech conference might get questions like, what time is the keynote? Or who's presenting in the AI panel? Through embeddings, the ChatBot can understand that these inquiries are about session schedules or speaker details. Most importantly, embeddings help AI figure out the context, like whether the word bark refers to a tree or a dog based on the conversation. This makes AI seem more human, able to chat with us, suggest songs or recommend movies and shows that we'll enjoy. As you've learned, embeddings convert text into numerical vectors that AI models can easily work with helping interactions with them feel more human. With this knowledge, you're ready to learn more about the technical details of embeddings, and how they are created and used in AI powered apps.

Chapter: 1. Text Embeddings
Topic: Create embeddings with OpenAI
Transcript: Embeddings are numerical vectors that help AI understand what words mean and how they relate to each other. They're a big part of what makes AI-powered tools smart, performing common tasks like searching, comparing, recommending, generating text and more. So how are embeddings created and how do generative AI models use them? Well, I'll start by teaching you how to create embeddings using the OpenAI API. OpenAI provides powerful and highly trained models to generate embeddings. The process involves feeding a word, sentence, or entire document into the model which then returns a vector that captures the text's essence, meaning, and context. That's exactly what you're going to start doing now. So let's get hands on and see how this works in practice. First, make sure that you have your API key from OpenAI to start. Once you have that, you can use OpenAI's embedding model. All right, so to generate embeddings with the OpenAI API, you send text to the embeddings API endpoint specifying which embedding model to use. Currently OpenAI offers three embedding models as you can view here in the docs with text-embedding-3-large and text-embedding-3-small being their latest third generation models. For this course, I'm going to use the model text-embedding-3-small which has an output dimension of 1,536. More on that part in just a bit. You'll notice how the API docs provide examples and code snippets for creating and embedding what's JavaScript using its node API library or SDK which I've already installed as a dependency in our project. If you have a look at the project files package.json file, you can see that OpenAI has been installed as a dependency. Okay, so this snippet here will get me started with generating embedding. So what I'll do is copy this main function right from the docs and paste it in my text editor, here in the file main.js. Now currently this snippet is using the embedding model text-embedding-ada-002. So I'll update it to use the newer model which is text-embedding-3-small just like that. The input text is currently a string which I'll update to say, "Hi, I'm your teacher, Guil!" And you can optionally set the format to return the embeddings using this encoding format parameter. By default, it's set to float, but you can also set it to something like base64, but I'll keep it as float for this course. And what this means is that it's going to return the embeddings as floating point numbers as you'll see soon. And this main function logs the embedding response to the console, so I'll run this code and open up the developer tools, and notice how in the console we get back a response object containing the data with the embedding. Now this response holds a lot of useful information, as you'll see, like the token usage and model, but what I'm looking for is this part here, the embedding vector. As you can see, it's a regular JavaScript array holding a massive list of floating point numbers. So just so we can have a better look at it, I'll access and log only the embedding portion of the response with embedding.data. I want to access the object at index zero and it's embedding property. And there it is, the actual embedding for the text, "Hi, I'm your teacher, Guil!" Now something that's really important to know about embeddings is the concept of dimensions or how many numbers are inside the embeddings array. As I mentioned, the text-embedding-3-small model I'm using supports a dimension of 1,536. So yes, this means that there are that many floating point numbers inside this array which you can confirm here in the array length. And actual text input size doesn't matter at all. So for example, if I changed this input to just, "Hi," you'll notice how the embedding array's length or dimension is still 1,536. Now these numbers may not mean much to us, but for AI, this embedding array holds important semantic details about the input text. In this case, all of the different semantic or contextual aspects of the words hi. And if I use the larger model, text-embedding-3-large, for example, it would mean double that, a dimension size of 3,072 as you can see here in the docs. Now OpenAI's embedding models are not just for single sentences. You can also pass it an array of strings and it will generate an embedding for each string. So I'll go ahead and try this with an array of conference session descriptions. So now I'll need to pass this session's array as the input like so and then I'll log the value of the data array to the console just like that. So what the function is going to do is send this array of strings to the embedding model and then return a set of high dimensional vectors for each string. Let's have a look. Okay, so now in the console, I see a list of five embedding objects each containing an embedding array, nice. So with OpenAI's embedding model, a word, sentence, or even entire documents can be reduced to a vector which can be used to compare text, search through large volumes of text, or even feed it into another AI model to generate responses or summaries. Now it is important to be mindful of token usage when using OpenAI's embedding models. Each piece of text you input into the model consumes tokens. So do check out their pricing page to learn more about the latest information on embeddings pricing. All right, so well done. You've just unlocked a fundamental skill in generative AI, creating embeddings with the OpenAI API.

Chapter: 1. Text Embeddings
Topic: Pair embeddings with related text
Transcript: You've learned some of the basics of embeddings and even created your own using an OpenAI embedding model. Currently you're left only with the embedding data for each string in the sessions array. So how do you actually use this data or feed it to an AI? Well, first pairing each embedding with its corresponding text chunk would be useful. This will also help when you begin storing these vectors in a database which you'll learn more about later in this course. So this will involve iterating over the data in the sessions array and placing each string in an object with its corresponding embedding, and this time, I've added more session titles to the array. Notice how there are 10 strings consisting of session titles. Now there are various ways you might do this. I'll cover one approach using the JavaScript for-of loop. First, inside this main function, I'll initialize a data variable to an empty array. Then right below, set up my for-of loop with for const, let's say textChunk of text, and this main function will accept a parameter named text. Then I'll place the call to the OpenAI embeddings endpoint within my for-of loop just like that. Once again, I'll use the text-embedding-3-small model. Then set the value of input to the current value of textChunk in the for-of iteration and keep the encoding format set to float. Now through each iteration of the for loop, we're going to push a object into our data array. So right below we'll say data.push and pass it an object, and this object is going to have two properties. One will be named content and this will be set to the current textChunk value, and then we'll have a values property which will be the embedding. And the way we can access the embedding is again, with embeddingResponse.data, and I'll access the object at index zero, and it's embedding property just like that. All right, right below, I'll console.log the value of data and then just to make sure that everything went okay, I'll log a message to the console that says Embeddings complete. All right, so this approach processes each input textChunk sequentially within the loop, ensuring that only one API request gets handled at a time which can be useful for small data sets and managing API rate limits, for example. So finally, I'll call my main function and pass it my sessions array as the input, run the code, and let's have a look. Good, I get the the Embeddings complete message. I'll open up the array and the console and there you have it, each embedding alongside its respective textChunk. So in the first object, we have the values properties set to the embedding, for the session title, Welcome Ceremony and Keynote. Then in the second object, we have the embedding for AI and Education, Developing a Data Strategy, and so on. As I mentioned earlier, this pairing brings structure to the data, preparing it for other uses like storing and managing these pairs in a database.

Chapter: 1. Text Embeddings
Topic: Text chunking for embeddings
Transcript: So far, you've created embeddings from short sentences or session titles. Well, working with a large amount of text or even entire documents poses a challenge, especially when your goal is to capture as much context as possible in a piece of text. For instance, in the latest project files, I've included a text file in this documents folder named sessions.txt, and it contains more detailed information about the Red30 Tech conference, such as the session dates, prices, lodging information, and all the sessions and speakers. Now, generating a single embedding for an entire document like this can result in the loss of critical nuanced details within the context or details that are crucial for the AI's understanding and interpretation. Also, most text embedding models like those provided by OpenAI have a token limit. OpenAI's limit, for instance, is 8,191 tokens, which is equivalent to about 5500 words. So it's best to break up large amounts of text into smaller individual chunks so that you don't exceed these limits. And there are many tools out there that can chunk documents for you. I'll teach you how to split text using LangChain, which is a popular framework in AI development, and you can learn a whole lot more about it at langchain.com. Now, before chunking, it is important to pre-process your data. So this means cleaning up your text to remove any irrelevant information like HTML tags or miscellaneous symbols, if you're copying your content from a website, for instance, also correcting typos and standardizing the formatting. Now, LangChain offers several tools for splitting your text, and one useful splitter is the CharacterTextSplitter, which lets you split text based on specific characters called a separator, and then it counts chunk length by the number of characters you set. And overall, it provides a really simple way to create manageable chunks for the embedding model. For this course, I'll use LangChain's RecursiveCharacterTextSplitter, which is much better suited for more complex documents. What it does is it iteratively splits your text into optimal sizes while trying to keep semantically related sentences and paragraphs together as long as possible. To use this text splitter, I'll need to install LangChain in my project. So in the terminal, I'll go ahead and stop running the project, then run npm install langchain. All right, I'll run my project again with npm run dev. There we go. And then I'll import the RecursiveCharacterTextSplitter from LangChain here in my main digest file. In fact, you can refer to the LangChain documentation for that code, which is this import statement here. So I'll copy that and paste it into Main Digest. And for this, I'll go ahead and remove the sessions array from my file, because now we are going to be bringing in the text from the sessions.txt file here in the documents folder. All right, so this code snippet creates a new instance of RecursiveCharacterTextSplitter and then splits the text you provide. To start, I've defined a function in the latest project files named split Text. So what I'll do is copy this entire splitter snippet into my function. And the text splitter configuration includes the chunk size and chunk overlap parameters. The chunk size influences how much text each chunk contains, and the chunk overlap ensures continuity and context between chunks with a bit of text from the end of one chunk repeated at the beginning of the next, as you'll see soon. So what I need to do first is access the content from the document I want to split, which is the sessions.txt file. Now for this front end project, I'll use the fetch method with await fetch to make a request to get the file sessions.txt. So I'll pass fetch a string containing the path to the sessions.txt file, and I'll save the fetch response in a const named response. All right, next, I'll convert the response object into plain text with await response text, and I'll store the resulting text from sessions.txt and a constant named text. Okay, now let's move on to the splitter configuration. What I'll do for this project is set the chunk size to a small value like 125, and the chunk overlap to 15. So this means that it's going to take the last 15 characters of one chunk, and it's going to repeat those characters at the beginning of the next chunk. And it does that to help preserve some context and continuity between chunks. The output variable is going to hold the split text, so to test this, I'll log the value of output to the console. Finally, I'll call my split text function, then run my code and head over to the browser and check out the console. All right, so I'll view the app in the browser, open up the console, and good. Notice how it logs an array with 55 document objects. Each has a page content property containing the text chunk, and also a metadata property, which lets you know the location of the text chunk in the original document, which could be useful in some cases. And you'll notice how each chunk is roughly 155 characters, including the character overlaps or the text shared between adjacent chunks. Now with the RecursiveCharacterTextSplitter, your chunks won't all exactly be the chunk size and overlap you've set, but the splitter does try to make them consistent in size. Selecting the best chunk size and overlap depends on several factors, like the embedding model limitations, the expected user queries, and how you plan to use the text chunks. So I always recommend experimenting to find the right settings for your project. Aim to produce the smallest chunk size possible while preserving as much context or information. So for instance, if you can look at a text chunk and make sense of it without surrounding context, there's a good chance that a language model can understand it too. Text chunking is important, especially when dealing with volumes of text. As you'll learn by breaking down documents into smaller chunks, AI models can better understand and capture the context and nuances of the text.

Chapter: 2. Vector Databases
Topic: The power of vector databases
Transcript: You've taken your first steps in transforming sentences and even entire documents into text embeddings using tools like LangChain and powerful embedding models like those offered by OpenAI. So now you're dealing with a massive number of complex embeddings, remember, each with a dimensionality of 1536, and you need a place to store these. Normally, when developing data-powered apps, you'll store data in a MySQL or Postgres database. But traditional databases like these aren't necessarily equipped to handle such complex, high-dimensional data efficiently. The solution? Vector databases. Unlike traditional databases that rely on exact keyword matching to retrieve information, vector databases use advanced similarity metrics to quickly find vectors in the database that are closest to a given vector. Vector databases use all the information encoded in embeddings to determine how closely related two vectors are based on their dimensions and values. This allows AI systems to find the most relevant vectors to a user query with extraordinary accuracy, matching content using all of the information encoded in embeddings. So what is the role and significance of embeddings and vector databases in generative AI tools and applications? Well, even the most advanced AI models, especially those based on large language models like GPT, have a limitation. They're only as good as the data they were last trained on. This means there could be gaps in their knowledge, which can lead to outdated or imprecise responses, not to mention the occasional tendency to hallucinate or generate incorrect or nonsensical information under certain conditions. So vector databases address this by letting you store custom, up-to-date information as embeddings. That way your AI applications can provide responses tailored to the latest and most relevant information, further augmenting the capabilities of pre-trained or foundational models. So now imagine the possibilities of building AI-powered tools that are well-informed about a specific domain or custom dataset, like a contract, legal document, your company product details, and more. Vector databases are a big deal in the world of generative AI. They do all the heavy lifting and math calculations to help developers make the most of embeddings, searching and finding data based on its context and meaning, not just exact matches.

Chapter: 2. Vector Databases
Topic: Set up a vector database with Pinecone
Transcript: You learn that vector databases store and query embeddings quickly end at scale. In this course I'll use Pinecone, A vector database designed to handle high dimensional vectors like the ones you've created. Pinecone is ideal for building AI-powered apps, requiring fast and accurate searches, like recommendation engines, chatbots, user support tools, and more. To begin, head over to pinecone.io and sign up for an account. Pinecone offers various plans to suit different needs, including a free tier that's perfect for getting started and experimenting with vector databases. I've signed up for the free plan and recommend you do as well for now. Once you've signed up, you'll gain access to the Pinecone Console where you can create and manage vector databases. Pinecone indexes store records with vector data. To create your first index directly in the Pinecone console, click Create Index. Start by giving the index a name like red30-tech-conf notice how the name can only contain lowercase letters, numbers, and hyphens. When configuring your index, the dimensions must be consistent with the embeddings you store. For instance, clicking setup by model shows that open AI's text embedding model has a dimensionality of 1,536, so enter 1,536 for dimensions. Pinecone uses a distance metric to calculate the similarity between vectors. I see that open AIs model requires a co-sign similarity metric. So select co-sign for the metric value. You'll learn more about this metric shortly. Under capacity mode, you'll have a serverless index by default, if you are on the free tier. Serverless indexes scale automatically based on usage, and you pay only for the amount of data stored and operations performed. Now, currently with the free tier, you get up to two gigabytes of storage, which is more than enough to get you started without paying. And by default, the cloud provider is AWS, and it's going to choose a region closest to you when on the free starter plan. Finally, click Create Index. Then your first index should be up and running shortly. There we go, it's all set up. Okay, next, you'll need to connect your application to your index, so you can store and query vectors from the app. You'll need an API key for that. So click API keys and the sidebar nav, and you'll see that Pinecone has already set a default key to get started. Be sure to keep this API key handy because soon you'll use it along with these code snippets shown here to install Pinecones node client in your project, initialize it, and then write vectors into your index.

Chapter: 2. Vector Databases
Topic: Store embeddings in Pinecone
Transcript: You've set up a Pinecone account and index, the next step is to store embeddings in Pinecone in preparation for something called similarity searches. Pinecone offers a REST API for storing and querying your vector data. But to streamline your development process, I recommend using Pinecone's node client. It provides the easiest way to create and interact with indexes. First, be sure to download the very latest project files with this video, where I've already installed and set up the Pinecone library and the project using the install and initialize snippets provided by Pinecone. So with your API key in hand, you'll need to initialize your Pinecone client here. And I'm also targeting my red30-tech-conf index here and importing Pinecone in my main.js file as well as the Pinecone index for red30-tech-conf. Okay, so you've already created an index through the Pinecone dashboard earlier. Now, you can test your connection to Pinecone using the listIndexes method, which returns information about all indexes in your project. So I'll just test it here with console.log, then call it with await pinecone.listIndexes. And in the console I get back information about my only index, red30-tech-conf. I see the index name, dimension, metric, and most importantly, that it's ready to go. Now remember, with the free starter plan, you get one project and up to five indexes for that project. And the starter plan also does not support all Pinecone features, which I'll talk more about soon. All right, now I'll generate new embeddings from text chunks, again, using the text in the sessions.txt file and then store them in Pinecone. This splitText function should look familiar. It's going to split the text in the provided text file or document using LangChain's RecursiveCharacterTextSplitter. This time I'm using a chunk size of 200 characters and a chunk overlap of 20 characters. Next, the generateEmbeddings function will loop over the array of text chunks, passing each to OpenAI's embedding endpoint to convert each chunk into an embedding. And again, as you've seen in previous videos, I'm pairing each text chunk with its corresponding embedding. Now, when storing vectors in Pinecone, you need to structure each vector set like so, a unique ID that's a string, and a values property, holding the vectors. Pinecone also lets you attach metadata key value pairs to vectors. So I'll add a property named metadata and set it to an object holding the content property containing the current text chunk. And there are various ways you might generate a unique ID. What I did for this lesson here in the utils folder is create a simple function that increments by one each time it's run and then returns the number as a string. I'm already importing the function here in my main.js file, so I can call it as the value of the ID property, just like that. And this generateEmbeddings function is going to return the final data array. And now I can begin storing vectors in my index using Pinecone's upsert method. Upsert is the process of inserting records if they don't exist or updating them if they do. So here's how you can upsert a batch of vectors into your index. I'll start by defining an async function named upsertRecords. Inside the function is where I'll call my generateEmbeddings function with await generateEmbeddings, and I'll pass it the path to the file I want to create embeddings from. So that is this file here. Remember it's /documents/sessions.txt, which I can reference with my txtFile variable. And I'll store the embeddings and a const named embeddingsData. Next, I'll upsert the records into my Pinecone index with await, then call index.upsert and then I'll pass it my embeddingsData, just like that. And finally, just to make sure that the upserting was successful, I'll log a message to the console that says, Upsert Successful. All right, now I'll call my upsertRecords function to upsert all of the records into my index. I'll run my code and let's have a look at the console. Okay, we get the message Embeddings complete! So that part went well and good. I see the console message, Upsert Successful! After adding or updating records, I recommend checking that your vectors are correctly stored and available for querying. Now, there might be a brief delay before you can access all of the latest vector data, but you can verify the current state using Pinecone's describeIndexStats method. So here I'll quickly console.log that with await, then say, index.describeIndexStats. And that's going to return stats about everything stored in your index. You can also quickly verify that your latest upsert was successful by checking the Pinecone console. And it was, good! Here I see that the vector count is 39 and I can view all of the stored records below. Perfect. Now, when dealing with multiple records that represent chunks of a single parent, like in my project, it helps to prefix each records ID with a reference to that parent doc. For example, name your IDs like so, red30TechConf#1, red30TechConf#2, #3, #4, and so on. And keep in mind that when inserting or updating larger amounts of data, Pinecone recommends inserting records in batches of 100 or fewer over multiple requests. Now, in my case, I inserted less than 100 records in this case, 39, so I'm okay. And earlier I mentioned that Pinecone's free starter plan doesn't support all Pinecone features. If you're curious, some of the features include configuring your index to scale horizontally and vertically, and choosing different regions for your environments. Also, Pinecone indexes all metadata by default, meaning it organizes and prepares the metadata to make it quickly searchable and accessible using metadata filtering. And when metadata contains many unique values, like your text chunks, they'll likely consume more memory. So another feature that is not available in the starter plan is keeping memory utilization low with a feature called selective metadata indexing. Please review the Pinecone's documentation to learn a whole lot more about this. For now, keep your chunk sizes small. It's not so much of an issue with the document and index size you're using now while learning these concepts. And you can always upgrade your Pinecone plan when you're ready. Okay, so with your data indexed, you're now ready to query vectors and perform similarity searches.

Chapter: 2. Vector Databases
Topic: What is semantic search?
Transcript: Semantic search enhances how you interact with AIs and how they understand text and make meaningful connections between pieces of information. Semantic search is different from traditional search methods because it's not about matching specific keywords in a database, for example. It's all about understanding the intent and meaning behind a user's search query. Traditional search, known as Lexical search, excels at finding exact matches, but is ineffective when queries are phrased differently from the stored data. Take email for instance. Imagine trying to find an email about a project update several months ago, but can't remember the exact phrases used in the message. So you type in variations of "Project update" into the search bar only to see a flood of emails, none of which are the ones you're looking for. This is where traditional keyword based search falls short. With Semantic search in place, you can search by meaning instead of phishing with specific keywords. If you can't remember the context of the message, you might type, "Important tasks before vacation," or, "Last month's marketing campaign discussion," which will yield more relevant results. The system doesn't just try to match the words important task, campaign, or discussion. Instead, it uses the semantics or the meanings and relationships behind your query. It understands that last month refers to a specific timeframe and that you're looking for a conversation, not just strict mentions of the words marketing campaign. While the emails that surface might not contain the exact string of words you entered, they are contextually related to your request. For example, that's how an app like Gmail can pull up an email where, "I'm leaving for a trip soon," was used instead of strictly, "Tasks before vacation," or, "Promotional strategies versus marketing campaign." Now think about everyday life and how you might remember a conversation with a friend or family member. You likely won't remember it word for word, but maybe through how it made you feel or a few details that may point to the overall theme of the conversation, like a joke, a planned trip, or advice they gave you. Semantic search aims to bring this level of understanding to AI, allowing systems to interpret queries with the same nuance and context as human conversation. This technique is applied in generative AI using tools you've learned, like embedding models trained on massive amounts of text that learn how words and concepts relate, and also with vector databases and text generation models, which you'll learn more about later in the course. The impact of semantic search is far-reaching. Users get more relevant responses with less effort. For example, you can easily find products and services that match your needs and content providers can better reach their intended audience, ensuring their content resonates and engages with them and so much more.

Chapter: 2. Vector Databases
Topic: Send queries to Pinecone
Transcript: A key part of interactingwith AI systems is retrieving and respondingwith information that's most relevantto what the user asked.You've stored embeddings and text in your Pinecone index.The next step is to query the data.So now I'll teach youhow Pinecone facilitates semantic search,and you're going to start bringing everything togetherby wiring up the code to the chatbot UI.In the latest project files with this video,I'm referencing DOM elements like the form,input field and the chat reply div,where the responses will be displayedand assigning them to variables,and this form event listener is goingto run some code on submit and displaya reply here in this section of the chatbot UI.Pinecone helps you find the most similar recordsto a search query by analyzinga vector representation of the searchand providing the closest matches basedon similarity scores.You use the query operationor method in Pinecone to search the indexusing your provided query vector,and the query method returns the closest records rankedby their similarity to the query vector.So notice in the docs how the query method returnsan array of objects containing matches.Each object has an ID and similarity score,and you can also retrieve the vector valuesand metadata for these results.Now, the similarity score is oneof the most important parts.It shows how similar or close a vector isto a query vector basedon a distance metric used by Pinecone.For instance, you set up your indexto use the cosign metric.With that metric, scores closerto one are more similar to your query,and scores nearing negative one representcomplete dissimilarity, suggesting the vectors are opposite.Understanding these metrics is keyfor interpreting your query results correctly.I'll start by sending a queryto Pinecone using its query method.The query method takes three important parameters,your query vector with the vector parameterand how many results you want backusing the topk parameterand if you wish to include vector values in your query.So first, as you've done before,I'm using OpenAI's embedding modelto convert a query into a vectoror embedding here in the generateEmbedding function.This function is going to take the value passed for input,run it through the embedding modeland return the high dimensional embedding.Next in the query data function is where I'll ask Pineconeto find the most similar embeddings among the index data.I'll set that up here with const queryResponseand then call the query method on the indexwith await index.query and pass it an objectwhich will hold our parameters.First, I'll set the vector parameter to the query vectorthat will come back from OpenAI's embedding endpoint.Remember that's the value of our query vector parameter,and I'll first return the top matching vectorby setting the topk parameter to one,and I do not need to include the vector valuesin the response.So I'll set the includeValues parameter to false,and you do want to include the metadata,which remember, is going to hold the matching text chunkor the text that's closest in meaning to your query.So I'll set the value of includeMetadata to true.I'll also log the response matches to the consoleto help me understand the details of the response.I can get that with queryResponse.matches,and finally, this function needsto return the text chunk returned from Pinecone,and I can access that from the metadata like so.For now, I'll only return one match or the first itemin the matches array as indicated here,but soon you'll learn how to handle multiple matches.Okay, it's time to bring everything together herein the form event listener.Remember, first you needto convert the user's question into an embedding.So here I'll call my generateEmbedding function,passing it the value of the input field.So that's the user's question,and I'll assign the embedding to the const queryEmbedding,then I'll pass the query embeddingto the queryData function to query the Pinecone indexand find a relevant matchfrom the sessions data we stored earlier.So I'll pass queryData, the value of queryEmbeddingand assign the matching data to the const reply,and finally, I'll display the reply in the UIwithin a paragraph element like so,chatReply.innerHTML, and then I'll set thatto a template literal containing P tags holding the reply,just like this.Alright, now I'll try it out.First, I'll try asking what is the cost to attend?Run the code and good.The search does match this query by sayingthat the admission price is $250and then the early price of $200,even though the query did not explicitly mention anythingabout admission or price.It just said what is the cost to attend.In the console, we see the matching arrayand notice how the similarity score is 0.4.Okay, I'll try a few more examples.So let's say where do I sleep?For instance.Run that and good.It tells me that the nearby hotels are Pixel Palace,Code and Comfort and The Digital Loft.How about something like how to stay safe?For instance,and I should get back something relatedto the security session, and I do.Awesome.Alright, so let's try one more.Let's say last session of the conference and good.I get back the matching text chunkfor the closing ceremony, even though I just askedto list the last session.Okay, but how about returning multiple matches?Well, I'll try setting the value of topk to threeand then, back in my chatbot, I'll provide a querylike sessions Sam Altman would like.Sam Altman, as you might know, is the CEO of OpenAI.So naturally, I expect the response to be about AI sessions,and yes, Pinecone returns textabout a session related to AI,and in the console, I get three matching text chunks,each somewhat related to the topic of AI.Okay, how about asking it something completely unrelatedto Red30 Tech Conf, like I want to cook.Well, sometimes when a database has limited contentand only one description related to food,like healthy and hearty lunch options and snacks.It might be the closest matchto any food related query.So Pinecone's algorithm still attemptsto find the closest match basedon the semantic relationships,even if the context is directly unrelated.So there is some work that needs to be doneto get a more accurate, engagingand conversational response,like one you might expect from a chatbotand bring together multiple matches into one reply,but this is essentially a big part of what'sbehind the way users communicate with AI,is using conversational text.So imagine how you might use this approach to searchthrough contracts, reports, company notes and documentationand easily find relevant information, ask questionsand get replies based on the context of the documents.Before moving on, I want you to take the timeto experiment with different queriesand explore the potential of semantic searchwith OpenAI embeddings and Pinecone,and now would also be a great time for youto add notes about what you're learningabout text embeddings and vector databasesin your course notebook.

Chapter: 3. Conversational Responses with OpenAI and Pinecone
Topic: Retrieval-augmented generation (RAG)
Transcript: Retrieval-augmented generation. I bet these words sound highly technicaland intense at first.Still retrieval-augmented generation is a common approachthat AI engineers use to enhance AI systemsand make them respondwith highly relevant information conversationallyand interact in a human-like way.So what do I mean by that?Retrieval-augmented generation, commonly referred to as RAG,is a framework in AIand natural language processing that merges two core steps,as the name states, retrieval and generation.So I'll break these down to help youunderstand how they work.In the retrieval phase,information gets pulled or retrievedfrom a large data source like a vector databaseto find contextually-relevant data based on a search query.This phase is important for putting togetherthe information needed for the next step, generation.In the generation phase,a language model or text generation model like GPTsteps in to generate a responseusing the retrieved information as context.The model produces responses that are not only relevant,but also coherent, creative, and engaging.Okay, so why are two phases necessary?You see, vector databases like Pineconeare highly optimized to retrieveinformation like matching vectorsand text chunks using similarity search.But as you've seen,it cannot generate new contentor do anything elsebut retrieve the actual data matching a query.On the other hand,a generative model like GPT can produce creative text,but might fall short when it comesto factual accuracy or domain-specific knowledge.Most times you'll need your AI-powered appto produce accurate and creative responseslike summaries, suggestions, or insightsfrom the retrieved text.So RAG bridges this gap by combining the strengths of both.It leverages the accuracy of embeddingsand vector database retrieval with the creativityand ability of generative models.By feeding the content retrieved from the databaseinto GPT, for example,RAG enables the creation of accurateand creative responses tailored to the query.And this is how many AI toolsare conversational, up-to-date, and contextually accurate.RAG is everywhere and used in numerous fieldsfrom enhancing AI applications across customer serviceto education and research.For instance, chatbots provide personalized answers.Educational tools support students with tailored resources.Researchers can quickly analyze extensive literature,get summaries, and so much more.And there you have it.You've unlocked a crucial AI skillby grasping the powerof retrieval-augmented generation or RAG.In upcoming lessons, you'll apply this understandingto augment the text chunk data retrieved from Pinecone.

Chapter: 3. Conversational Responses with OpenAI and Pinecone
Topic: Create replies and manage messages with OpenAI
Transcript: You learn that at its coreretrieval-augmented generation, or RAG,combines the accuracy of data retrievalfrom databases like Pineconewith the creativity of generative models like GPT-4.This process enables an AI to find relevant informationand respond in a way that feels personal and contextual.Imagine you're looking for informationabout the nearest hotels.You know that Pinecone can retrieve text chunks relatedto your query using embeddings and similarity search.While direct matches provide accurate information,the goal is to create engaging and informative responses.You're retrieving relevant text from Pinecone,now it's time for the generation phasewhere you'll breath some life into the datausing a generative model like GPT-4.You'll elevate AI interactions from data retrievalto engaging conversations.You'll create dynamic repliesand manage messages between a user and the AI.So, to recap, this code transforms a user queryor input into an embedding, capturing its essenceand lots of contextual information.Then we're using Pinecone here to sift through the indexto find the vectors and text chunksthat best match the query embedding.And this chunk is going to form the basisof the generated reply.In the latest project files with this videoI have added this main function to bring everything togetherfrom generating the queryEmbeddingto finding a match in Pineconeand generating the final chat completion,which you'll do in this new function here.So, with all the necessary context being returnedI'll use GPT-4 and the generateChatCompletion functionto generate a response.As I mentioned at the beginning of this course,I'm going to assume that you've worked with this modeland know the basics of using the OpenAI API'sChat Completions endpoints.I'll start by adding the usual setof instructions within a messages array.The first object in this array contains a system messagewith instructions for GPT-4,setting the tone for the AI's persona.I'll set the role property to system,and the content property to a system prompt.The prompt reads, "You are a friendly assistantwho supports people interested in signing upto attend Red30Tech Conf.You will be given two pieces of information,some context about the conference and a question.Your job is to formulate a short answerusing the provided context.If you are unsure and cannot find the answer in the context,say, 'Sorry, I don't know.'Please do not make up the answer."Next, in the generateChatCompletion functionI'll push a new object into the messages arraywith messages.push.This object needs to have a role set to user.And in the initial prompt I inform the model to analyzethe provided context about the conferenceand the user question.So, this time the content property will hold both the valuepassed in for text and query just like this.And this should be enough to allow the modelto use the context or the matching text chunkto form a tailored response.Next, I'm making a call to OpenAI's Chat Completions API,and I'll feed it the system and user messagestored in the messages array by assigningthe messages parameter the value of messages,or simply passing it messages since both are the same.And this is going to ask GPT-4 to continue the conversation.And here you have the option to carefully adjust parameterslike temperature and frequency penaltyto balance creativity with coherence.The frequency_penalty parameter adjusts the outputbased on the frequency of certain terms.By increasing the frequency penaltyyou lower the likelihood of the model repeating phrases,making it sound more human.I'll set it to 0.5.And the temperature parameter takes a valuebetween zero and two.Setting temperature to 0.5, for example,could help make the AI's conversationmore natural yet predictable.All right, so now GPT-4 is going to usethe provided user query and matching text,along with the system instructionsto generate an informative and engaging reply.So, up in my main functionI'll call await generateChatCompletion,and this function takes the user inputand the matching text chunk return from Pinecone.So, I'll pass it the current value of the match variableand the value passed for the input parameter.And the content return by this functionwill be displayed in the chat bot as the reply.And similar to what we did earlier,the form EventListener is going to callthe main function on submit,passing it the submitted user questionor the value of the input field.All right, now it's time to test it.I'll first try asking a question likewhere can I stay during the conference?It's thinking, and then I get back a conversational replythat says, "You can stay at nearby hotelssuch as Pixel Palace, Code & Comfort, or Digital Loftduring the conference."Awesome.How about asking it are there any sessions about music,and let's see what it replies.Okay, good, it says that, "Yes, there is a sessioncalled 'The Art of Sound.'"Now, how about, now, I did instruct itthat if it did not know the answer to reply,"Sorry, I don't know the answer."So, next I'll ask it something related to food.I'll say I don't eat meat, and let's see.Aha, it says that, "We will be providing vegan varietiesfor lunch at the conference and snacks,so there will be options available for you."Now, I did instruct the model that if it wasn't sureof the answer or if it could not find it in the contextto reply, "Sorry, I don't know the answer."So, I'll try asking it something completely unrelatedlike what is the temperature at the event?And remember that the context or the knowledge basedoes not have any information regarding temperature,so let's see.And fantastic, it replies, "Sorry, I don't know the answer."All right, wonderful, you've transformed a simple queryinto a rich conversational experience.Queries about 3D printing lead to recommendationswith insights into the speakers and duration.Inquiries for price discounts suggest when to sign upfor the best price and so on.So, that's the power of combining OpenAI's GPT-4with Pinecone's retrieval capabilitiesto create AI-driven interactions.This is a big milestone in your AI development journey.So, I encourage you to experiment with queriesand explore conversational AI with RAG,and imagine how you can applythese techniques to your projects.And as always, don't forget to add notesabout what you've learned in the course notebook.

Chapter: 3. Conversational Responses with OpenAI and Pinecone
Topic: 
Transcript:

Chapter: 3. Conversational Responses with OpenAI and Pinecone
Topic: 
Transcript:

Chapter: 4. Conclusion
Topic: 
Transcript:





